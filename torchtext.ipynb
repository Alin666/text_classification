{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [基础torchtext数据预处理] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n",
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchtext\n",
    "from torchtext import data,datasets\n",
    "#import spacy  也可以换成spacy分词工具，242上安装了spacy，246上安装了pyltp.\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "list_stopwords=list(set(stopwords.words('english')))\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchtext.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitsentence(text):\n",
    "    sentences=sent_tokenize(text)\n",
    "    return sentences\n",
    "def splitword(sentence):\n",
    "    words=word_tokenize(sentence)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is tom.', 'He has a big head.', 'I like soccer!']\n",
      "['My', 'name', 'is', 'tom', '.']\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    print(splitsentence(\"My name is tom. He has a big head. I like soccer!\"))\n",
    "    print(splitword(\"My name is tom.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x7f59f4ff3d90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT=data.Field(sequential=True,tokenize=splitword,lower=True,fix_length=150,stop_words=list_stopwords,batch_first=True)\n",
    "# LABEL=data.Field(sequential=False,use_vocab=False)  #官方文档的写法\n",
    "LABEL=data.LabelField(dtype=torch.long)\n",
    "\n",
    "train,val,test=data.TabularDataset.splits(path='./ose_data_2/',\n",
    "                                         train='train_2.tsv',\n",
    "                                         validation='validation_2.tsv',\n",
    "                                         test='test_2.tsv',\n",
    "                                         format='tsv',\n",
    "                                         skip_header=True,\n",
    "                                         fields=[('Label',LABEL),('Text',TEXT)])\n",
    "#TEXT.build_vocab(train)\n",
    "#from torchtext.vocab import Vectors\n",
    "#vectors = Vectors(name='/home/yuling/tyling-data/word_vectors/glove.6B.100d.txt')\n",
    "# 指定 Vector 缺失值的初始化方式，没有命中的token的初始化方式\n",
    "from torchtext.vocab import GloVe\n",
    "TEXT.build_vocab(train,val,vectors='glove.6B.100d')\n",
    "LABEL.build_vocab(train,val)\n",
    "# 查看词表元素\n",
    "TEXT.vocab.vectors\n",
    "vocab=TEXT.vocab\n",
    "vocab\n",
    "#TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(train)):\n",
    "#     print(train[i].Label)\n",
    "# for i in range(len(val)):\n",
    "#     print(val[i].Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 150]) tensor([[  851,   123,   184,  ...,   738,    30,  1603],\n",
      "        [  141,     2,  1164,  ...,  5516,  2748,     3],\n",
      "        [  102,  4144,   162,  ...,   433,   427,   522],\n",
      "        ...,\n",
      "        [  125,   888,   815,  ...,    25,   385,   527],\n",
      "        [  312,    55,  1696,  ...,     4,     3,   212],\n",
      "        [16976,  1612,  3668,  ...,     2,   154,     3]])\n",
      "torch.Size([32]) tensor([0, 2, 2, 2, 2, 0, 0, 2, 0, 0, 0, 1, 2, 2, 1, 2, 0, 1, 0, 1, 2, 2, 0, 2,\n",
      "        2, 1, 2, 0, 2, 2, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "train_iter,val_iter,test_iter=data.Iterator.splits((train,val,test),\n",
    "                                                   sort_key=lambda x:len(x.Text),\n",
    "                                                   batch_sizes=(32,256,256))\n",
    "                                                  #device=-1)print(train[5])\n",
    " #查看数据\n",
    "for batch_idx,(X_train_var,y_train_var) in enumerate(train_iter):\n",
    "    print(X_train_var.shape,X_train_var)\n",
    "    print(y_train_var.shape, y_train_var)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train[5])\n",
    "# print(train[5].__dict__.keys())\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [网络结构]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 与维度变换相关的函数 view() , permute() , torch.squeeze() , torch.unsqueeze()     \n",
    "2. Embedding层加载预训练模型的方式： 1) copy , 2) from_pretrained    \n",
    "3. LSTM 参数以及输入输出说明： 结构参数 LSTM(input_size,hidden_size,num_layers)   \n",
    "   input_size:输入的特征数量    \n",
    "   hidden_size:隐藏的特征数量     \n",
    "   num_layers:层数  \n",
    "4. Linear 的创建需要两个参数，inputsize 和 outputsize    \n",
    "   inputsize:输入节点数    \n",
    "   outputsize：输出节点数    \n",
    "   所以Linear有7个字段：    \n",
    "       weight: Tensor , outputsize x inputsize    \n",
    "       bias: Tensor , outputsize    \n",
    "       gradWeight: Tensor , outputsize x inputsize    \n",
    "       gradbias: Tensor , outputsize    \n",
    "       gradinput: Tensor    \n",
    "       output: Tensor   \n",
    "       _type: output:type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_matrix=TEXT.vocab.vectors\n",
    "len_vocab = len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Enet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Enet,self).__init__()\n",
    "        \n",
    "        self.embedding=nn.Embedding(len_vocab,100)\n",
    "        \n",
    "        #若使用预训练的词向量，需要指定预训练的权重;\n",
    "        #此处的pretrained_embeddings即为权重矩阵 weight_matrix;\n",
    "        #self.embedding = nn.Embedding.from_pretained(pretrained_embeddings,freeze=False)\n",
    "        \n",
    "        self.lstm = nn.LSTM(100,64,3,batch_first=True)#,bidirectional=True)\n",
    "        self.linear = nn.Linear(64,3)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        text = x\n",
    "        batch_size,seq_num = text.shape #[32,150]\n",
    "        vec = self.embedding(text)  #[32,150,100]\n",
    "        out,(hn,cn) = self.lstm(vec)  #[32,150,64]\n",
    "        out = self.linear(out[:,-1,:])  #[32,3]\n",
    "        out = F.softmax(out,-1)\n",
    "        \n",
    "        #print(\"x.shape:\",x.shape) [32,150]    \n",
    "        #32即为batch的大小，150为batch中每个样本的序列长度       \n",
    "        #print(\"vec.shape:\",vec.shape) [32,150,100]\n",
    "        #100即为序列中每个词向量的维度\n",
    "        #print(\"out.shape:\",out.shape) [32,150,64]\n",
    "        #因为设定的LSTM中词向量的维度是64，所以经过LSTM之后维度从100变为64\n",
    "        #print(\"linear_out.shape:\",out.shape) [32,3]\n",
    "        #经过全连接层之后降维\n",
    "        #经过softmax之后可以得出在每个分支上的概率\n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    7,    58,    78,  ...,    46,     2,  1260],\n",
      "        [ 2908,  2953,  2388,  ...,  6979,  1297,     2],\n",
      "        [19404,  1145,     2,  ..., 18367,     2,  5056],\n",
      "        ...,\n",
      "        [  233,  4142,  1664,  ...,    68,  6413,     3],\n",
      "        [ 8307,  2135,   329,  ...,   695,  2464,   354],\n",
      "        [   15,    13,    55,  ...,  4938,  1415,   879]])\n",
      "torch.Size([32, 150])\n"
     ]
    }
   ],
   "source": [
    "# 模型单独测试\n",
    "net = Enet()\n",
    "# net.to(device)\n",
    "for i in train_iter:\n",
    "    print(i.Text)\n",
    "    print(i.Text.shape)\n",
    "    net.forward(i.Text)\n",
    "    break\n",
    "#net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [训练]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(filter(lambda p:p.requires_grad, net.parameters()),lr=0.01)\n",
    "loss_function = F.cross_entropy   #用交叉熵作为loss优化的评判标准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(1.0971, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(train_iter):\n",
    "    print(batch_idx)   \n",
    "    optimizer.zero_grad()\n",
    "    predicted = net(batch.Text)\n",
    "    #print(predicted)\n",
    "    #print(batch.Label)\n",
    "    loss = loss_function(predicted, batch.Label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TextRNN 模型] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_dim, hidden_size, num_layers, bidirectional, pretrained_embeddings):\n",
    "\n",
    "        super(TextRNN, self).__init__()\n",
    "    \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = output_dim   #output_dim可以表示输出维度，也可表示标签个数\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        #self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim,\n",
    "                            self.hidden_size, \n",
    "                            self.num_layers, \n",
    "                            batch_first=True, \n",
    "                            #第一个维度设为batch，即（batch_size，seq_length，embedding_dim）\n",
    "                            bidirectional=self.bidirectional)\n",
    "        self.fc = nn.Linear(self.hidden_size*2, output_dim)  #本例中一律用双向\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        text = x\n",
    "        batch_size , seq_length = text.shape\n",
    "        #print(\"batch_size:\",batch_size)\n",
    "        #vec = self.dropout(self.embedding(text))\n",
    "        #vec = vec.permute(1,0,2)\n",
    "        vec = self.embedding(text)\n",
    "        #print('\\nvec.shape:',vec.shape)\n",
    "        #以下为初始化h0 和 c0\n",
    "        h0 = torch.zeros(self.num_layers*2,batch_size,self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers*2,batch_size,self.hidden_size)\n",
    "        \n",
    "        lstm_out, (hn,cn) = self.lstm(vec,(h0,c0))\n",
    "        \n",
    "        #print('\\nlstm_out.shape:',lstm_out.shape)\n",
    "        #我们只需要最后一步的输出，即（batch_size,-1,output_size）\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        #print('fc_out.shape:',out.shape)\n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "embedding_dim = 100\n",
    "output_dim = 3\n",
    "hidden_size = 64\n",
    "num_layers = 5\n",
    "bidirectional = True\n",
    "#dropout = 0.5\n",
    "\n",
    "model = TextRNN(embedding_dim, output_dim, hidden_size, num_layers, bidirectional, pretrained_embeddings)\n",
    "\n",
    "for i in train_iter:\n",
    "    model.forward(i.Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [导入TextCNN模型进行训练]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 332,  678, 1358,  ...,  670,  853, 1359],\n",
      "        [1550,  481, 4737,  ..., 3795, 8195, 2942],\n",
      "        [1185,  804, 1084,  ..., 2574, 1139,  445],\n",
      "        ...,\n",
      "        [3980, 2336,  129,  ...,  480, 6923, 2030],\n",
      "        [7752, 2315, 4795,  ..., 1104,    6,  313],\n",
      "        [8821,  807, 8578,  ...,  113, 4917,   83]]) torch.Size([32, 150])\n",
      "tensor([[  233,   437,  1664,  ...,   716,   271,  2322],\n",
      "        [   33,  1672,     2,  ..., 20198,  1452,     2],\n",
      "        [  119,     2,   510,  ...,  2936,  8117,  2353],\n",
      "        ...,\n",
      "        [ 1052, 15096,   123,  ...,     2,  8081,  1480],\n",
      "        [ 4172,  1356,     2,  ...,  3822,  1725,  2527],\n",
      "        [ 2281,    84,   153,  ...,     2,  6072,  4560]]) torch.Size([32, 150])\n",
      "tensor([[  141,  6133,   545,  ...,  5395,     3,  1035],\n",
      "        [ 1942,   391,  2908,  ...,  1078,   231,    70],\n",
      "        [   62,   882,     2,  ..., 19451,   156,  2680],\n",
      "        ...,\n",
      "        [ 1741,  2473,   360,  ...,   201,     6,   552],\n",
      "        [   36,   413,   305,  ...,     3,  4582,  2458],\n",
      "        [  470,     2,  4357,  ...,  8351,  1052,   112]]) torch.Size([32, 150])\n",
      "tensor([[ 4739,     7,   119,  ...,   632,   108,    85],\n",
      "        [ 2024,  6159,   184,  ...,     5,   193,   526],\n",
      "        [  372,    23,  1375,  ...,  1489, 14651,  1124],\n",
      "        ...,\n",
      "        [   84,   340,  1074,  ...,    11,   129,   654],\n",
      "        [  207,  6039,     3,  ...,     2,    15,  1053],\n",
      "        [  121,    18,   342,  ...,  5379,     7,  4827]]) torch.Size([32, 150])\n",
      "tensor([[  30,  224,    5,  ...,    3, 6712, 1740],\n",
      "        [ 695, 9064,    3,  ..., 5702, 2598,    5],\n",
      "        [3886, 2661, 6161,  ...,    4,    3,  543],\n",
      "        ...,\n",
      "        [2908, 2953, 2388,  ..., 6979, 1297,    2],\n",
      "        [  37,   55, 2403,  ..., 6900,  865,  338],\n",
      "        [7558, 7533, 2736,  ...,  123,  108,    2]]) torch.Size([32, 150])\n",
      "tensor([[ 2138,  1045,  2074,  ...,   352,   208,   276],\n",
      "        [ 2974,   148,   235,  ...,    12,  4506,     2],\n",
      "        [  121,    18,   342,  ...,  4841,     3,  5508],\n",
      "        ...,\n",
      "        [  494,  3055,  5116,  ...,  2309,   229,  1345],\n",
      "        [ 3129,     2,   128,  ...,  1388,   932,   174],\n",
      "        [ 2660,  9818, 11126,  ...,  1405,  6508,  3941]]) torch.Size([32, 150])\n",
      "tensor([[ 756,    2,  156,  ...,  383,   99,  534],\n",
      "        [4710, 1326, 9064,  ..., 2521, 1093, 1835],\n",
      "        [3843,   54, 1997,  ..., 6651, 3923,  724],\n",
      "        ...,\n",
      "        [ 113, 4115,    2,  ...,  624,   16, 5127],\n",
      "        [ 330,   88,  152,  ..., 1573,    3,  104],\n",
      "        [3980, 2336,  129,  ...,  912, 6913,    2]]) torch.Size([32, 150])\n",
      "tensor([[1320,  245,  224,  ...,  240, 5328,   18],\n",
      "        [  40,  735, 1917,  ...,   12, 5088, 1249],\n",
      "        [ 211,   89, 1639,  ..., 7029,  210, 2572],\n",
      "        ...,\n",
      "        [ 451, 1956,    4,  ..., 1937,    2,   45],\n",
      "        [ 261,   93, 3878,  ...,    3,   61,   54],\n",
      "        [2051,  957,    7,  ...,   40,   30, 1457]]) torch.Size([32, 150])\n",
      "tensor([[ 2121,   901,   283,  ...,    32,   464,    14],\n",
      "        [ 6423,  2318,  1425,  ...,   338,  1218,  2893],\n",
      "        [   93,  1757,   600,  ...,     3,   707,   995],\n",
      "        ...,\n",
      "        [  851,   123,   184,  ...,  2907,   112,   103],\n",
      "        [  262,    73,     9,  ...,   806,     5,    31],\n",
      "        [  460,    23, 10640,  ...,   872,     3,   635]]) torch.Size([32, 150])\n",
      "tensor([[ 1716,  1425,   236,  ...,   466,     2,    19],\n",
      "        [ 3313,  2153,   121,  ...,   961,  1781, 18930],\n",
      "        [ 1154,   361,  2337,  ...,   112,  7655,  2531],\n",
      "        ...,\n",
      "        [ 1196,  2764,     2,  ...,     3,    11, 14584],\n",
      "        [ 2164,   561,  1643,  ...,   414,  6920, 10441],\n",
      "        [ 4357,    52,   815,  ...,  8589,  8351,  3526]]) torch.Size([32, 150])\n",
      "tensor([[ 1291,   145,   157,  ...,  9369,    94,     2],\n",
      "        [ 1293,     2,     5,  ...,   137,     5,   313],\n",
      "        [ 1928,   148,   230,  ...,  7832,   971,     3],\n",
      "        ...,\n",
      "        [ 1928,   148,   230,  ...,  2218, 17751,   247],\n",
      "        [ 7558,  7533,  2736,  ...,    27,   292,  1959],\n",
      "        [  207,    33,   136,  ...,     3,   186,    14]]) torch.Size([32, 150])\n",
      "tensor([[   24,   170,  3202,  ...,  2216,  2302,   956],\n",
      "        [  265,  6621,    42,  ...,  1466,  2258,   187],\n",
      "        [ 1491,  1484,  5364,  ..., 13194,   106,    60],\n",
      "        ...,\n",
      "        [   20,  2396,   116,  ...,  8016,   160,    20],\n",
      "        [    7,    58,    78,  ...,   509,     2,    92],\n",
      "        [   44,    98,   218,  ...,  9593,     9,  1596]]) torch.Size([32, 150])\n",
      "tensor([[   28,  1474,    95,  ...,  1216,   495,  4006],\n",
      "        [19404,  1145,     2,  ..., 18367,     2,  5056],\n",
      "        [   98,  1694,   602,  ...,  7221,     2,  6971],\n",
      "        ...,\n",
      "        [ 2133,    55,     2,  ...,   371,     8,  1612],\n",
      "        [ 1716,   780,   236,  ..., 18679,   239,   175],\n",
      "        [ 3207,  1117,  2553,  ...,  8090,  1403,   517]]) torch.Size([32, 150])\n",
      "tensor([[ 531, 2994,  248,  ...,    3, 1694, 4865],\n",
      "        [ 847, 7744, 2751,  ...,   37,  288,  804],\n",
      "        [1164,  343,  570,  ..., 4707, 2737, 1792],\n",
      "        ...,\n",
      "        [ 245, 1320,    5,  ...,  173,  414,  419],\n",
      "        [1247, 2034,  885,  ...,  127,   14,    2],\n",
      "        [ 847, 7744, 2751,  ...,  576, 1540, 1123]]) torch.Size([32, 150])\n",
      "tensor([[  123,  2872,     2,    32,   317,  2675,  7848,     2,     5, 12712,\n",
      "          1603,   210,  4888,   155,   659,   158,  2824,  3019,    16,  1892,\n",
      "          1778,    17,     3,    82,  2072,  5507,     2,   120,   129,  9906,\n",
      "          1892,     2,  1256,     2, 20044,  8303,   324,  8074,     2,   454,\n",
      "           757,   959,   160, 12038,  1344,  9289,     2,   250,   267, 13350,\n",
      "          1892,  1344,     2,  3835,   811,     2,  9840,  9344,  5616,     3,\n",
      "          9978,   615,  1085,   811,     2,  3474,     2,  3152,  2930,  8531,\n",
      "             3,   236,   754,  1514,    10, 13307,   659,     2,   324,  1344,\n",
      "             2,  1514,     2,  1778,   123, 19119,     2, 17583,   139,  4428,\n",
      "         11339,   763,   293,   657,     2,     9,  2824,  6859,   499,     2,\n",
      "          5180,  8705,  1344,    74,     2,     9,  9106,  8635,  5615,  2247,\n",
      "             3,  9581,    19,   324,  1344,     2,    12,   140,     3,    52,\n",
      "            22,     2,   540,   252,  3585,   790, 17135,  2824,  6859, 17702,\n",
      "             2,   659,  1010,    25,  1778,    82,  1179,     3,   208,   110,\n",
      "             6,  2882,    86,     3,     2,  2282,   122,     3,   200,    48],\n",
      "        [ 6797,   506,  4585,    19,  4292,  5292, 13075,  2324,   830,     2,\n",
      "          4391,    30,   310,  1208,     3,  4391,   391,     2,  2776,   439,\n",
      "           566,  4585,   591,     2,  5292,     9,     3,  1122,     2, 16304,\n",
      "         15515,    10,  1179,     3,   391,  4391,     2,   566,  2776,   439,\n",
      "          1098,   813,  9138,     3,  4391,     2,    65,     7,  2620,    87,\n",
      "           149,  2269,     2,  1231,     2,   538,   844,     3,   831,   149,\n",
      "         12870,   591,   137,  2776,   439,   244, 12336, 14072,   294,  3171,\n",
      "         18021,   127,  9608,  5659,    99,  3641,  3315,   151,    20,  1171,\n",
      "          1685,     3, 13859,  1270,   472,  2324,   830,   783,  1031,     3,\n",
      "           160,  3791,   247,  3780,  7016,     2,  6602,     2, 13980,  2248,\n",
      "          2966,  1158,     2,  5103,    95,  3315,     3,    27,   223,  2776,\n",
      "           439,   654,  2280,  4691,  4744,  1998,   541,   149,   506,   831,\n",
      "          1981,     3,  2415,     2,     4,  4292,     2,  3171,  1306,  1802,\n",
      "           417,   831,  1399,  6862,   619,   717,    18,  8033,  1769,  2888,\n",
      "         11626,  6825,  4744,  2851,    93,  1006,  1300,   885,    16,  4542],\n",
      "        [ 1082,  1478,  1451,  4144,   162,  1278,  2974,   148,   235,   153,\n",
      "          5500,  5430,     3,    10,  1278,  6323,   119,  1866, 12032,  1246,\n",
      "           280,   118,   374,     3,  5674,   912,  7836,     2,   591,   102,\n",
      "         10658,     2,   770,   954,   588,    41,   280,   912, 16911,   315,\n",
      "          4185,     3,    98,  1681,   986,   596,     2,   231,  8022,   109,\n",
      "          3197,  1552,  1082,  1393,     2,  8489,    41,  2948,  4966,  6799,\n",
      "           986,     2,  3121,  6723,  2773,  4279,  1114,  2228,   374,  7843,\n",
      "          1805, 11707,   168,  8162,    12,  6193,  4144,  4900,     2,   223,\n",
      "         10200,  3441,   354,     2,  3676,  1643,  3236,    12,  4506,     2,\n",
      "           179,   130,    47,     3,  8894,  9122,   255,   180,   430,    12,\n",
      "          1985,   268,   225,   143,   424,  6018,  9733,   427,  7806,  1037,\n",
      "          1297,  7295,     2,   119,   789,   400,   826,   922,     2,     2,\n",
      "            24,    18,     2,   245,  1278,     3,  8716,  1554,  2853,   364,\n",
      "          2438,  5414,  9747,     7,  1894,   512,     2,   179,  9471,  1709,\n",
      "          6343,  4867,  3111,  9365,     3,  4867,   179,   295,   341,  7907],\n",
      "        [  492,  2097,  2991,  5965,  2848,     2,  1448,  1209,     3,  1726,\n",
      "         10849,     2,  1079,    25,   385,    33,   112,  2311,     3,  1844,\n",
      "           741,   586,  2730,     3,    12,   109,    97,  1571,  9702,  1580,\n",
      "           152,     2,  5403,   569, 11898,   150,   342,     2,    75,   720,\n",
      "          3060,  2613,    16,  4947,    17,   543,    73,     4,    27,   195,\n",
      "             8,   732,    39,  2116,  2060,   109,  1897,   250,  6703,     3,\n",
      "          2116,  2060,   109,  1706,     5,  4816,  4034,  2116,  2635,  3413,\n",
      "             3,   838,  1346,  6717,  5339,  1448,     2,   171,  4815,   377,\n",
      "          1415,  6585,  2765,   564,   234, 20134,  1370,  2227,  8783,     9,\n",
      "          1383,   149,  5339,   109,   682,   496,  4431,  1740,   322,   314,\n",
      "           293,   388, 11802,  1024,   314,  5926,   569,  2123,     3,    33,\n",
      "            38,    55,  1415,  1455,     3,    38,    55,  2707,  3679,     2,\n",
      "          3679,   834,     2,   170,  9923, 11423,  5686,  2060,   109,  4431,\n",
      "           968,  1455,  3255,     2,   151,  1316,    56,   170,    37,    37,\n",
      "           659,     3,     2,   259,     2,   328,   355,  2284,   860,   505],\n",
      "        [  565,   179,  1806,   337,   145,   238,  1105,   270,   130,   221,\n",
      "            14,     3,    83,   928, 15330,   238,   999,   492,   382,   565,\n",
      "            39,    60,   281,    39,  3558,   875, 10068,   106,    93,   270,\n",
      "          1074,    16,  3767,    17,   194,    20,     5,   149,   565,     2,\n",
      "          2338,    13,     2,  3558,   875,   288,   565,     3,    13,   149,\n",
      "            97,   565,     2,   565,  1061,   123,    20,  1000,     3,    42,\n",
      "           565,  4834,     2,    32, 16228,  1984,  3178,  5114,     2,   378,\n",
      "           232,  3767,     2,     4,   372,   565,    39,  1122,   791,    47,\n",
      "             2,   589,    42,     2,   565,  1766,   337,   405,   186, 20304,\n",
      "           921,   292,  2740,    41,   750,    20,     2,  1431,   337,   875,\n",
      "             2,  1767, 16315,     3,  3767,     9,   546,   286,  1562,    20,\n",
      "           565,    39,   130,   221,    14,     2,   286,   179,  9264,    13,\n",
      "           145,   182,   238,  2206,     3,   546,   238,   565,  8682,     2,\n",
      "          2977,   238,   565, 11637,  8672,  6804,   565,   273,    73,     2,\n",
      "            20,  7845,  5547,   945,   565,   193,    47,  2184,   599, 13798]]) torch.Size([5, 150])\n"
     ]
    }
   ],
   "source": [
    "from TextCNN import TextCNN\n",
    "\n",
    "#测试模型\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "embedding_dim =  100\n",
    "output_dim = 3\n",
    "n_filters = 64  #卷积核的个数\n",
    "filter_sizes = [2,3,4]\n",
    "dropout = 0.7\n",
    "\n",
    "model = TextCNN(embedding_dim, n_filters, filter_sizes, output_dim, pretrained_embeddings, dropout)\n",
    "for i in train_iter:\n",
    "    print(i.Text,i.Text.shape)\n",
    "    model.forward(i.Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [导入TextRCNN模型进行训练]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-c5119722e52a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextRCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tyling-data/English-HAN-classification/TextCNN.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embedding_dim, output_dim, hidden_size, num_layers, bidrectional, pretrained_embeddings, dropout)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTextRCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, embeddings, freeze, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;36m4.0000\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m5.1000\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m6.3000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0;34m'Embeddings parameter is expected to be 2-dimensional'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "from TextCNN import TextRCNN\n",
    "\n",
    "#测试模型\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "embedding_dim = 100\n",
    "output_dim = 3\n",
    "hidden_size = 64\n",
    "num_layers = 5\n",
    "dropout = 0.5\n",
    "\n",
    "model = TextRCNN(embedding_dim, output_dim, hidden_size,num_layers, pretrained_embeddings, dropout)\n",
    "for i in train_iter:\n",
    "    print(i.Text, i.Text.shape)\n",
    "    model.forward(i.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
